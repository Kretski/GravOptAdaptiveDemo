import torch
from GravOptAdaptiveE_demo import GravOptAdaptiveE  # –ò–º–ø–æ—Ä—Ç –æ—Ç obfuscated

# –ü–æ-–≥–æ–ª—è–º –º–æ–¥–µ–ª –∑–∞ –ø–æ–≤–µ—á–µ –∑–∞–º—Ä—ä–∑–≤–∞–Ω–µ
model = torch.nn.Linear(10, 10)  # 200 –ø–∞—Ä–∞–º–µ—Ç—ä—Ä–∞ ‚Äì –ø–æ-–¥–æ–±—Ä–µ –∑–∞ —Ç–µ—Å—Ç
optimizer = GravOptAdaptiveE(model.parameters(), lr=0.01, freeze_percentile=30)

print("Diagnostics: median_grad =", torch.median(torch.abs(list(model.parameters())[0].grad)).item() if list(model.parameters())[0].grad is not None else "N/A")  # –¢–≤–æ—è—Ç custom print (–∞–∫–æ –∏—Å–∫–∞—à)

for step in range(100):
    x = torch.randn(32, 10)  # –ü–æ-–≥–æ–ª—è–º batch –∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    y = x ** 2  # –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–π –∑–∞ —Å—Ç–∞–±–∏–ª–Ω–æ—Å—Ç
    if step == 50:
        y = x ** 3
        print("üîÑ –ü–†–û–ú–Ø–ù–ê –ù–ê –ó–ê–î–ê–ß–ê–¢–ê!")
    y_hat = model(x)
    loss = torch.mean((y_hat - y) ** 2)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    if step % 20 == 0:
        print(f"–°—Ç—ä–ø–∫–∞ {step}: Loss = {loss.item():.2f}")

print("‚úÖ –¢–µ—Å—Ç –∑–∞–≤—ä—Ä—à–µ–Ω ‚Äì –≤–∏–∂ –ø—Ä–∏–Ω—Ç–æ–≤–µ—Ç–µ –∑–∞ –∑–∞–º—Ä—ä–∑–≤–∞–Ω–µ/—Ä–∞–∑–º—Ä–∞–∑—è–≤–∞–Ω–µ!")